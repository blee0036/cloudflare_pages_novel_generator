# 预处理脚本内存配置说明

## 核心机制：分批处理 + 内存限制

为防止处理大量书籍时内存耗尽导致系统卡死，预处理脚本提供了**分批处理**和**内存限制**双重保护：

### ⭐ 分批处理机制

- **默认**: 每批最多处理 **100本** 书
- **低内存**: 每批最多处理 **50本** 书  
- **高内存**: 每批最多处理 **200本** 书

**重要**: 处理完一批后，**再次运行相同命令**即可继续处理剩余书籍。每次运行都会：
1. 自动跳过已处理的书籍
2. 合并之前的处理结果
3. 只处理未处理或有更新的书籍
4. 限制单批处理数量，防止内存溢出

---

## 使用方式

### 1. 默认配置 (推荐)
```bash
npm run preprocess
```
- **内存限制**: 512MB
- **批次大小**: 100本/批
- **适用场景**: 普通书籍（单本 < 20MB），一般电脑
- **处理速度**: 正常

**示例输出**:
```
找到 250 本书
其中 0 本已是最新，250 本需要处理
本次将处理: 100 本

[1/100] 正在处理《示例小说》...
...
[100/100] 完成...

📌 注意: 还有 150 本书待处理
请再次运行 "npm run preprocess" 继续处理剩余书籍
```

**再次运行**:
```bash
npm run preprocess  # 处理第2批（100本）
npm run preprocess  # 处理第3批（50本）
# 直到所有书籍处理完成
```

---

### 2. 低内存配置 (最安全)
```bash
npm run preprocess:low-memory
```
- **内存限制**: 256MB
- **批次大小**: 50本/批
- **适用场景**: 
  - 小型书籍（单本 < 10MB）
  - 内存紧张的设备（2-4GB RAM）
  - 低配服务器、虚拟机
  - 后台运行，不想占用太多资源
- **处理速度**: 较慢，但最安全

**推荐使用场景**: 
- 内存 < 4GB 的设备
- 后台运行时不想影响其他程序
- 有上百本书需要处理

---

### 3. 高内存配置
```bash
npm run preprocess:high-memory
```
- **内存限制**: 1024MB (1GB)
- **批次大小**: 200本/批
- **适用场景**:
  - 大型书籍（单本 > 30MB）
  - 高配置设备（>= 8GB RAM）
  - 需要快速处理大量书籍
- **处理速度**: 最快

**推荐使用场景**:
- 内存 >= 8GB 的设备
- 需要快速完成大批量处理
- 单本书籍文件较大（>20MB）

---

### 4. 自动循环处理（实验性）
```bash
npm run preprocess:all
```
- 自动循环运行，直到所有书籍处理完成
- 每批之间有2秒延迟，让系统释放资源
- **注意**: 目前需要手动停止（Ctrl+C）

---

## 内存监控功能

脚本运行时会自动监控内存使用情况：

### 监控信息示例
```
开始预处理...
批处理设置: 每批最多处理 100 本书
内存使用: 堆内存 10MB / 512MB, 总内存 65MB

[1/100] 正在处理《示例小说》 - 作者名 (15MB)
[1/100] 完成《示例小说》，共 1234 章。
内存使用: 堆内存 120MB / 512MB, 总内存 185MB
```

### 警告信息
当内存使用超过80%时，会自动触发垃圾回收：
```
⚠️  警告: 内存使用率过高 (85%)，尝试GC...
✓ GC完成，当前堆内存: 350MB
```

### 大文件警告
处理超过30MB的书籍时会提示：
```
⚠️  注意: 这是一个大文件 (45MB)，处理可能需要较多内存和时间
```

---

## 实际测试数据

以下是真实环境测试结果（处理4本书，共6660章）：

### 单批处理（4本书）
| 书籍 | 文件大小 | 章节数 | 峰值堆内存 | 峰值总内存 |
|------|---------|--------|-----------|-----------|
| 《BUG之神》 | 1MB | 673 | 10MB | 105MB |
| 《DOTA之最强血脉》 | 2MB | 1284 | 16MB | 122MB |
| 《GT病毒进化者》 | 2MB | 1268 | 22MB | 139MB |
| 《从零开始》 | 16MB | 3435 | 62MB | 213MB |

### 分批处理测试（批次=2本）
| 批次 | 处理书籍数 | 峰值内存 | 说明 |
|------|-----------|---------|------|
| 第1批 | 2本 | 121MB | 处理完后内存116MB |
| 第2批 | 2本 | 213MB | 自动跳过已处理的2本 |

**结论**: 
- 小说（<5MB）: 内存占用通常 < 50MB
- 中型小说（5-20MB）: 内存占用 50-150MB
- 大型小说（>20MB）: 内存占用可能 > 150MB
- ✅ **分批处理后，内存不会累积**
- ✅ **每批处理完都会释放内存**
- ✅ **多次运行会自动合并结果**

---

## 如何选择合适的配置

### 根据书籍数量
| 书籍数量 | 推荐配置 | 说明 |
|---------|---------|------|
| < 50本 | 默认配置 | 一次处理完成 |
| 50-200本 | 默认配置 | 需运行1-2次 |
| 200-500本 | 低内存配置 | 需运行4-10次，更安全 |
| > 500本 | 低内存配置 | 分多次运行，每次50本 |

### 根据系统内存
| 系统总内存 | 推荐配置 | 原因 |
|----------|---------|------|
| < 4GB    | low-memory | 防止系统卡死 |
| 4-8GB    | 默认配置 | 平衡速度和安全 |
| >= 8GB   | high-memory | 快速处理 |

### 根据书籍大小
| 平均文件大小 | 推荐配置 |
|------------|---------|
| < 10MB | low-memory（50本/批） |
| 10-20MB | 默认配置（100本/批） |
| > 20MB | high-memory（200本/批），但批次可能更少 |

---

## 使用示例

### 场景1: 首次处理300本书（低配电脑）
```bash
# 第1次：处理前50本
npm run preprocess:low-memory
# 输出: 📌 注意: 还有 250 本书待处理

# 第2次：处理第51-100本
npm run preprocess:low-memory  
# 输出: 其中 50 本已是最新，250 本需要处理
#       本次将处理: 50 本
#       📌 注意: 还有 200 本书待处理

# ... 重复6次，直到所有书处理完
```

### 场景2: 更新部分书籍（已有200本，新增10本）
```bash
npm run preprocess
# 输出: 找到 210 本书
#       其中 200 本已是最新，10 本需要处理
#       本次将处理: 10 本
#       处理完成
```

### 场景3: 内存充足，快速处理
```bash
# 一次性处理200本
npm run preprocess:high-memory
```

---

## 优化建议

1. **首次处理大批量书籍**: 
   - 使用低内存配置，分多次运行
   - 关闭其他程序，确保系统流畅

2. **日常更新**: 
   - 使用默认配置即可
   - 只会处理新增或修改的书籍

3. **监控输出**: 
   - 留意内存使用警告
   - 如果频繁出现80%警告，降低批次大小

4. **自定义批次**: 
   - Windows: `$env:BATCH_SIZE="30" ; npm run preprocess`
   - Linux/Mac: `BATCH_SIZE=30 npm run preprocess`

5. **夜间处理**: 
   - 可以在系统空闲时运行
   - 使用默认或高内存配置加速

---

## 技术说明

- 使用 `--max-old-space-size` 限制 Node.js 堆内存
- 使用 `--expose-gc` 允许手动触发垃圾回收
- 每本书处理完后自动执行 GC
- 超过阈值时主动触发 GC
- 使用流式写入，避免内存累积
- 只保存章节索引而非完整内容，大幅减少内存占用
- **分批处理机制**: 通过环境变量 `BATCH_SIZE` 控制
- **自动合并**: 每次运行都会保留之前的处理结果

---

## 故障排除

### 问题1: 提示"还有X本书待处理"
**原因**: 这是正常现象，批次限制保护内存  
**解决**: 再次运行相同命令继续处理

### 问题2: 内存限制设置后仍然占用过高
**原因**: RSS（常驻内存）包含堆外内存  
**解决**: 
1. 降低 `BATCH_SIZE`：`$env:BATCH_SIZE="20" ; npm run preprocess`
2. 使用低内存配置

### 问题3: 处理速度很慢
**原因**: 批次太小，或内存限制过低导致频繁GC  
**解决**: 
1. 如果系统资源充足，使用高内存配置
2. 增加批次：`$env:BATCH_SIZE="150" ; npm run preprocess`

### 问题4: 出现 OOM (Out of Memory) 错误
**原因**: 单本书过大，超过内存限制  
**解决**: 
1. 使用高内存配置
2. 单独处理大书：将大书籍文件临时移到其他目录
3. 临时提高限制：`node --expose-gc --max-old-space-size=2048 -r tsx/cjs scripts/preprocess.ts`

### 问题5: 多次运行后，已处理的书又被重新处理
**原因**: manifest.json 或 hash 计算异常  
**解决**: 
1. 检查 `generated/manifest.json` 是否存在
2. 检查书籍文件是否被修改（会导致hash变化）

---

## 常见问题

**Q: 可以中途中断吗？**  
A: 可以。按 Ctrl+C 中断，已处理的书籍会保存。下次运行会跳过它们。

**Q: 可以同时运行多个实例吗？**  
A: 不建议。可能导致文件冲突。

**Q: 批次大小如何影响性能？**  
A: 
- 批次越大：处理越快，但内存占用越高
- 批次越小：更安全，但需要多次运行

**Q: 如何查看还有多少书待处理？**  
A: 运行任意预处理命令，开头会显示统计信息。

**Q: 可以只处理特定的书吗？**  
A: 目前不支持。可以临时移走不需要处理的书籍文件。
